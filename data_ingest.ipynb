{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df56559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Original dataset shape: (20491, 2)\n",
      "Dataset columns: ['Review', 'Rating']\n",
      "Missing values per column:\n",
      "Review    0\n",
      "Rating    0\n",
      "dtype: int64\n",
      "Data types:\n",
      "Review    object\n",
      "Rating     int64\n",
      "dtype: object\n",
      "\n",
      "Handling missing values...\n",
      "After handling NaNs - shape: (20491, 4)\n",
      "Entries with missing reviews: 0\n",
      "Entries with missing ratings: 0\n",
      "Cleaning text...\n",
      "After removing empty reviews - shape: (20491, 5)\n",
      "Creating chunks...\n",
      "Processing row 0...\n",
      "Processing row 1000...\n",
      "Processing row 2000...\n",
      "Processing row 3000...\n",
      "Processing row 4000...\n",
      "Processing row 5000...\n",
      "Processing row 6000...\n",
      "Processing row 7000...\n",
      "Processing row 8000...\n",
      "Processing row 9000...\n",
      "Processing row 10000...\n",
      "Processing row 11000...\n",
      "Processing row 12000...\n",
      "Processing row 13000...\n",
      "Processing row 14000...\n",
      "Processing row 15000...\n",
      "Processing row 16000...\n",
      "Processing row 17000...\n",
      "Processing row 18000...\n",
      "Processing row 19000...\n",
      "Processing row 20000...\n",
      "Created 20517 chunks from 20491 reviews\n",
      "\n",
      "Chunk Statistics:\n",
      "Total chunks: 20517\n",
      "Average chunk length: 105.74 words\n",
      "Rating distribution:\n",
      "rating\n",
      "1    1423\n",
      "2    1797\n",
      "3    2188\n",
      "4    6051\n",
      "5    9058\n",
      "Name: count, dtype: int64\n",
      "Chunks with ratings: 20517\n",
      "Saving processed data...\n",
      "Data ingestion complete!\n",
      "Files created:\n",
      "- processed_chunks.csv (20517 rows)\n",
      "- processed_chunks.json\n",
      "- processing_metadata.json\n",
      "\n",
      "Sample processed chunks:\n",
      "                                                text  rating  chunk_length\n",
      "0  nice hotel expensive parking got good deal sta...       4            87\n",
      "1  ok nothing special charge diamond member hilto...       2           251\n",
      "2  nice rooms not 4 experience hotel monaco seatt...       3           224\n",
      "3  unique, great stay, wonderful time hotel monac...       5            92\n",
      "4  great stay great stay, went seahawk game aweso...       5           197\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup NLTK\n",
    "nltk_data_path = os.path.join(os.path.expanduser('~'), 'nltk_data')\n",
    "os.environ['NLTK_DATA'] = nltk_data_path\n",
    "\n",
    "try:\n",
    "    nltk.download('punkt', force=True)\n",
    "    nltk.download('punkt_tab', force=True)\n",
    "except:\n",
    "    print(\"Automatic download failed - proceeding to manual installation\")\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\"tripadvisor_hotel_reviews.csv\")\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Dataset columns: {df.columns.tolist()}\")\n",
    "print(f\"Missing values per column:\\n{df.isnull().sum()}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Handle NaN values strategically\n",
    "print(\"\\nHandling missing values...\")\n",
    "\n",
    "# For reviews: replace NaN with empty string to preserve rating data\n",
    "df['Review'] = df['Review'].fillna('')\n",
    "\n",
    "# For ratings: replace NaN with 0 (neutral) to preserve review data\n",
    "df['Rating'] = df['Rating'].fillna(0)\n",
    "\n",
    "# Create a flag to track which entries had missing data\n",
    "df['missing_review'] = df['Review'] == ''\n",
    "df['missing_rating'] = df['Rating'] == 0\n",
    "\n",
    "print(f\"After handling NaNs - shape: {df.shape}\")\n",
    "print(f\"Entries with missing reviews: {df['missing_review'].sum()}\")\n",
    "print(f\"Entries with missing ratings: {df['missing_rating'].sum()}\")\n",
    "\n",
    "# Enhanced cleaning function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Remove HTML tags if any\n",
    "    text = re.sub(r'<[^>]+>', '', str(text))\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Keep alphanumeric, spaces, and basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"Cleaning text...\")\n",
    "df['cleaned_review'] = df['Review'].apply(clean_text)\n",
    "\n",
    "# Remove completely empty reviews after cleaning\n",
    "df = df[df['cleaned_review'] != '']\n",
    "print(f\"After removing empty reviews - shape: {df.shape}\")\n",
    "\n",
    "# Enhanced chunking function\n",
    "def create_chunks(text, max_tokens=500):\n",
    "    if not text or text.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        sentences = sent_tokenize(text)\n",
    "    except:\n",
    "        # Fallback to simple split if nltk fails\n",
    "        sentences = text.split('. ')\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "            \n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length <= max_tokens and current_length > 0:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Create chunks with enhanced metadata\n",
    "print(\"Creating chunks...\")\n",
    "chunks_data = []\n",
    "total_chunks = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 1000 == 0:\n",
    "        print(f\"Processing row {idx}...\")\n",
    "    \n",
    "    chunks = create_chunks(row['cleaned_review'])\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_length = len(chunk.split())\n",
    "        if chunk_length < 5:  # Skip very short chunks\n",
    "            continue\n",
    "            \n",
    "        chunks_data.append({\n",
    "            'review_id': idx,\n",
    "            'chunk_id': i,\n",
    "            'text': chunk,\n",
    "            'rating': int(row['Rating']) if not pd.isna(row['Rating']) else 0,\n",
    "            'has_rating': not row['missing_rating'],\n",
    "            'chunk_length': chunk_length,\n",
    "            'source': 'tripadvisor_hotel_reviews.csv',\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'category': 'hotel_review'\n",
    "        })\n",
    "        total_chunks += 1\n",
    "\n",
    "print(f\"Created {total_chunks} chunks from {len(df)} reviews\")\n",
    "\n",
    "# Create DataFrame and save\n",
    "chunks_df = pd.DataFrame(chunks_data)\n",
    "\n",
    "# Add some statistics\n",
    "print(f\"\\nChunk Statistics:\")\n",
    "print(f\"Total chunks: {len(chunks_df)}\")\n",
    "print(f\"Average chunk length: {chunks_df['chunk_length'].mean():.2f} words\")\n",
    "print(f\"Rating distribution:\\n{chunks_df['rating'].value_counts().sort_index()}\")\n",
    "print(f\"Chunks with ratings: {chunks_df['has_rating'].sum()}\")\n",
    "\n",
    "# Save processed data\n",
    "print(\"Saving processed data...\")\n",
    "chunks_df.to_csv('processed_chunks.csv', index=False)\n",
    "\n",
    "# Also save as JSON for easier loading in other scripts\n",
    "chunks_json = chunks_df.to_dict('records')\n",
    "with open('processed_chunks.json', 'w') as f:\n",
    "    json.dump(chunks_json, f, indent=2)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'total_reviews': len(df),\n",
    "    'total_chunks': len(chunks_df),\n",
    "    'average_chunk_length': float(chunks_df['chunk_length'].mean()),\n",
    "    'rating_distribution': chunks_df['rating'].value_counts().to_dict(),\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'source_file': 'tripadvisor_hotel_reviews.csv'\n",
    "}\n",
    "\n",
    "with open('processing_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Data ingestion complete!\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"- processed_chunks.csv ({len(chunks_df)} rows)\")\n",
    "print(f\"- processed_chunks.json\")\n",
    "print(f\"- processing_metadata.json\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample processed chunks:\")\n",
    "print(chunks_df[['text', 'rating', 'chunk_length']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
